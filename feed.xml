<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://androsaba.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://androsaba.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-10-26T10:11:50+00:00</updated><id>https://androsaba.github.io/feed.xml</id><title type="html">blank</title><entry><title type="html">Cross-Validation Estimator</title><link href="https://androsaba.github.io/blog/2023/cross_validation_estimator/" rel="alternate" type="text/html" title="Cross-Validation Estimator"/><published>2023-10-23T00:00:00+00:00</published><updated>2023-10-23T00:00:00+00:00</updated><id>https://androsaba.github.io/blog/2023/cross_validation_estimator</id><content type="html" xml:base="https://androsaba.github.io/blog/2023/cross_validation_estimator/"><![CDATA[<h4 id="notations">Notations:</h4> <ul> <li>\(m\) - metric</li> <li>\(K\) - number of folds</li> <li>\(N\) - number of different training sample realizations (drawn from the true population)</li> <li>\(\overline{m^n}\) - average metric (over \(K\) folds) for a given training sample realization</li> <li>\(Var(m^n_k)\) - variance of a metric over \(K\) folds for a given training sample realization</li> <li>\(Var(\overline{m^n})\) - variance of an average metric (over different training sample realizations)</li> <li>\(Var(m)\) - True/population variance of a metric</li> </ul> <h2 id="bias">Bias</h2> <p>The goal of the cross-validation is to estimate the mean of a metric, \(\overline{m}\) and its variance, \(Var(m)\). Cross-validation gives a pessimistically biased estimate of a performance because most statistical models will improve if the training set is made larger. However, Leave-one-out cross-validation (LOOCV) is approximately unbiased, because the difference in size between the training set used in each fold and the entire dataset is only a single instance <a href="https://stats.stackexchange.com/questions/154830/10-fold-cross-validation-vs-leave-one-out-cross-validation">[ref]</a>.</p> <h2 id="variance">Variance</h2> <p>Intra-fold variance of a metric \(Var(m^n_k)\) is affected by the sample size as well as how diverse the sample is or in other words how different are validation folds from each other. The folds can be quite different easily in the low data regime if there are some instabilities in the sample, like outliers (for instance, if a given validation fold is very different from the training fold the value of \(m^n_k\) can be very different from the other values and thus will increase its variance). To factor out the sample diversity/instabilities from the estimation of \(Var(m)\) cross-validation is repeated \(N\) times (\(N\) is a large number, say, 5000) with different training samples. As a result we get the \(N\) number of \(\overline{m^n}\) allowing us to compute the mean of means and the variance of means, \(Var(\overline{m^n})\) and that is what is usually referred to when discussing the variation of the CV estimator. If training samples are independent \(Var(\overline{m^n})\) is lower than that in the case of the correlated training samples based on the argument mentioned below.</p> <h2 id="effect-of-model-stability">Effect of Model Stability</h2> <p>Before we dive into the details one should mention that CV does a good job if we are dealing with large samples but small sample regime (e.g. 40-60 records) is a different story. Number of folds and instabilities have a prominent impact on the metric estimations in the latter scenario.</p> <p>Intuitively one thinks that with increasing \(K\) we get more overlapped training samples and thus almost the same models within CV, training folds become highly correlated. They make similar predictions (almost the same prediction if \(K\) is very large, for example, if \(K\) is the same as the number of data points which corresponds to LOOCV) for a given validation sample. Therefore these estimates of a metric are not independent, they are correlated and consequently lead to higher variance, \(Var(\overline{m^n})\) <a href="https://stats.stackexchange.com/questions/61783/bias-and-variance-in-leave-one-out-vs-k-fold-cross-validation/358138#358138">[ref]</a> (why the mean of a correlated data has larger variance than that of the independent one is explained <a href="https://stats.stackexchange.com/questions/223446/variance-of-the-mean-of-correlated-and-uncorrelated-data">here</a>).</p> <p>On the other hand one can show empirically that \(Var(\overline{m^n})\) doesn‚Äôt always increase with increasing \(K\) rather it depends on the stability of a model and a sample. If we are given a sample with no instabilities and a model is also stable<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> \(Var(\overline{m^n})\) goes down with increasing \(K\) (LOOCV gives minimum or almost minimum variance) (see Fig. 1) while for the unstable case it reaches minimum for some intermediate value of \(K\) (LOOCV is not minimum anymore) <a href="https://stats.stackexchange.com/questions/61783/bias-and-variance-in-leave-one-out-vs-k-fold-cross-validation/358138#358138">[ref]</a>. So the difference is in the high \(K\) region (see Fig. 2)<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/cross_validation_variance/CV_Variance_NoInstabilities_60points.png" class="img-fluid rounded medium-zoom-image large-image" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig1. Stable version: $Var(\overline{m^n})$ (left) and $Var(m^n_k)$ (right) as a function of $K$ </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/cross_validation_variance/CV_Variance_Instabilities_60points.png" class="img-fluid rounded medium-zoom-image large-image" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig2. Unstable version: $Var(\overline{m^n})$ (left) and $Var(m^n_k)$ (right) as a function of $K$ </div> <p>In general the variance is smaller in a stable environment compared to the unstable one. In the stable environment a given training sample is representative of the true distribution and thus \(\overline{m^n}\) is going to have similar values for different realizations of the training sample resulting in lower variance compared to the unstable scenario. In the latter case a given realization of the training sample is not representative of the underlying distribution. Imagine there are some instabilities in the sample, say, outliers. In the small data regime they may have a big influence on the model. For instance, if there are no outliers in the current realization of a sample (say, when \(n=1\)) we get stable model that behaves differently compared to the model we get, let‚Äôs say, in the next iteration (\(n=2\)) because in that realization outliers are present which is causing the fitted function to become more wavy/fluctuating. Due to that reason, outputs of these two models (\(n=1\) vs \(n=2\)) are going to be different and therefore there is larger difference between \(\overline{m^1}\) and \(\overline{m^2}\) leading to the higher variance, \(Var(\overline{m^n})\).</p> <p>In the low data regime \(Var(\overline{m^n})\) is large for small \(K\) in both cases, stable and unstable environments. That is because when \(K\) is small the training fold becomes even smaller and the model becomes very unstable as it is more sensitive to any noise/sampling artifacts in the particular training sample used <a href="https://stats.stackexchange.com/questions/154830/10-fold-cross-validation-vs-leave-one-out-cross-validation">[ref]</a>.</p> <table> <thead> <tr> <th>Model</th> <th>With Increasing K</th> </tr> </thead> <tbody> <tr> <td>Stable</td> <td>\(Var(m^n_k)\) increases, \(Var(\overline{m^n})\) decreases</td> </tr> <tr> <td>Unstable</td> <td>\(Var(m^n_k)\) increases, \(Var(\overline{m^n})\) has <strong>U</strong> shape (first goes down then goes up, like parabola)</td> </tr> </tbody> </table> <p>Now we try to explain why \(Var(\overline{m^n})\) goes (almost) monotonically down with respect to increasing \(K\) (even in high \(K\) region) for a stable model and why it goes up with increasing \(K\) (in high \(K\) region) in the case of an unstable model:</p> <h4 id="stable-model"><u>Stable Model</u></h4> <p>Intuitive explanation for decreasing variance \(Var(\overline{m^n})\) in case of the stable model is that going from \(K\) to \(K+1\) intra-fold variance \(Var(m^n_k)\) is increasing but the means (\(m_1\), \(m_2\),‚Ä¶, \(m_N\)) get closer to each other because the training sample gets bigger (more stable) so we get more similar estimates (means) of a metric.</p> <h4 id="unstable-model"><u>Unstable Model</u></h4> <p>In case of the unstable model, initially variance is large because the training samples are very small and models are more unstable than for large \(K\) values. So increasing \(K\) makes models more stable, \(K\) is still small so intra-fold variance \(Var(m^n_k)\) is small and the means are closer to each other thus variance \(Var(\overline{m^n})\) goes down. At some point, for some intermediate value of \(K\) variance reaches its minimum and then it starts to go up. This happens because for larger \(K\) intra-fold variances get very large and they change the values of their respective metric means (moves them further apart from each other). The models become more stable but it can‚Äôt cancel out the effect of increasing \(Var(m^n_k)\) leading to larger \(Var(\overline{m^n})\).</p> <h2 id="summary">Summary</h2> <p>For large samples the effect of stability is not significant. In a low data regime, whether a large number of folds is better or not depends on the stability of a model and a sample. In the case of a stable environment LOOCV is better than k-fold CV because of low variance and bias. In the case of instabilities some smaller number of folds seems to have lower variance than LOOCV. But one has to remember that the error of the estimator is a combination of bias and variance. So overall which one is better depends also on the bias term.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p><span class="footnote"> Variance of the cross validation estimator is a linear combination of three moments. The terms ùúî and ùõæ are influenced by correlation between the data sets, training sets, testing sets etc. and instability of the model. A model is stable if its output doesn‚Äôt change when deleting a point from the sample. These two effects are influenced by the value of ùêæ which explains why different datasets and models will lead to different behavior <a href="https://stats.stackexchange.com/questions/325123/why-does-k-fold-cross-validation-generate-an-mse-estimator-that-has-higher-bias/358504#358504">[ref]</a> </span>¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2" role="doc-endnote"> <p><span class="footnote"> Getting the effect of increasing variance for large values of ùêæ is very senstive to the sample size and numeber of outliers. If a sample or number of outliers is larger their effect cancel out each other.</span>¬†<a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name>Andro Sabashvili</name></author><summary type="html"><![CDATA[Discussing the variance of the cross-validation estimator]]></summary></entry><entry><title type="html">Prediction Intervals for Aggregated Forecasts</title><link href="https://androsaba.github.io/blog/2023/prediction_interval_for_time_series/" rel="alternate" type="text/html" title="Prediction Intervals for Aggregated Forecasts"/><published>2023-10-09T00:00:00+00:00</published><updated>2023-10-09T00:00:00+00:00</updated><id>https://androsaba.github.io/blog/2023/prediction_interval_for_time_series</id><content type="html" xml:base="https://androsaba.github.io/blog/2023/prediction_interval_for_time_series/"><![CDATA[<h2 id="intro">Intro</h2> <p>I won‚Äôt go into details about why prediction intervals are important, we all know that. I just want to introduce a framework that will allow us to estimate a prediction interval for a single forecast, and then we will generalize it for aggregated forecasts. I started thinking about this problem when I was working on a sales forecasting model earlier this year. The model produced monthly forecasts, but the clients were interested in aggregated forecasts, such as annual sales of a store or forecasted annual sales for the entire shopping mall. I realized that I needed a way to estimate the prediction interval for aggregated forecasts, given that I only had a model that produced monthly forecasts.</p> <h2 id="distribution-free-approach">Distribution-free Approach</h2> <p>There are many approaches to building prediction intervals. Most of them are based on the residuals assumed that they are normally distributed. There are some fancy formulas that allow you to construct prediction intervals based on this normality assumption. However, we know that this assumption is often unrealistic and doesn‚Äôt hold in practice. I wanted to have a method that is distribution-free, so I decided to use a technique based on the bootstrapping approach. The basic idea is to simulate future paths of the variable of interest. This will allow us to estimate the distribution of the variable at any future time point. The technique can be used for supervised learning algorithms as well as time series algorithms.</p> <h2 id="simulating-future-paths">Simulating Future Paths</h2> <p>To generate a simulated path, we start with a forecast at the first time point in the horizon, \(T+1\), draw a random residual from the residual distribution and add it to the forecast. This gives us the first simulated value, \(y^*_{T+1}\). We then use the first simulated value as the input to the model to generate a forecast for the next time point in the horizon. We repeat this process until we have generated a sequence of simulated values for all of the future time points. This process is repeated multiple times to generate multiple simulated paths. This will give us a distribution of the variable of interesty at any future time point which is used to construct prediction intervals.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/prediction_intervals/Path_Equation.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="aggregation-across-different-dimensions">Aggregation Across Different Dimensions</h2> <p>We consider two types of aggregations:</p> <ul> <li>Aggregation across time axis is the process of combining forecasts for different time points into a single forecast. For example, if you have monthly forecasts for a year, you can aggregate them to get an annual forecast.</li> <li>Aggregation across time series is the process of combining forecasts for different time series into a single forecast. For example, if you have forecasts for sales of different products, you can aggregate them to get a forecast for total sales. Our method can be used to estimate prediction intervals for both, aggregation across time and aggregation across time series.</li> </ul> <h3 id="aggregation-across-time">Aggregation Across Time</h3> <p>For each simulated path, aggregate the monthly forecasts to obtain an annual forecast. This can be done by simply summing the monthly forecasts for each year (see equation below). As a result we are left with an array of \(N\) (number of simulated paths) elements for a given year, \((y_{1,year}, y_{2,year},..., y_{N,year})\). Estimate the lower and upper bounds of the prediction interval at the desired confidence level by computing \((1-\alpha)/2\)-th and \((1+\alpha)/2\)-th quantiles of the resulting distribution.</p> \[y^*_{s, 2027} = \sum^{12}_{m=1} y^*_{s,m/2027}\] <h3 id="aggregation-across-time-series">Aggregation Across Time Series</h3> <p>To estimate a prediction interval for an aggregated forecast across time series do a cross-section of the simulated paths for each time series at the time point of interest. Which gives us \(T\) (number of time series) arrays of length \(N\) (number of simulated paths in a single time series). Each array is a distribution of \(y\) at that time point. Therefore, we can say that we have a set of distributions of \(T\) random variables and the goal is to estimate the distribution of the sum of those random variables given that the distribution for each of them is known. One approach to do that is convolution but becomes infeasible when the number of random variables (time series) is large. More scalable and fast approach is Monte Carlo simulation: these arrays/distributions are put together in a matrix with columns being a distribution corresponding to a single time series. So, the matrix has an \(N \times T\) shape (\(N\) rows because there are \(N\) elements in a distribution since there are \(N\) paths, \(T\) columns - because we have \(T\) time series to aggregate). Then we sum the matrix entries horizontally resulting in the array of length \(N\) which is the estimation of the distribution of the aggregated forecasts. Estimate the lower and upper bounds of the prediction interval at the desired confidence level by computing quantiles of the resulting distribution.</p> \[\begin{bmatrix} y^{*,1}_{1} &amp; y^{*,2}_{1} &amp; y^{*,3}_{1} &amp; \cdots &amp; y^{*,T}_{1} \\ y^{*,1}_{2} &amp; y^{*,2}_{2} &amp; y^{*,3}_{2} &amp; \cdots &amp; y^{*,T}_{2} \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ y^{*,1}_{N} &amp; y^{*,2}_{N} &amp; y^{*,3}_{N} &amp; \cdots &amp; y^{*,T}_{N} \\ \end{bmatrix} \begin{matrix} \\ \stackrel{\sum_t}{\Longrightarrow} \\ \\ \end{matrix} \begin{bmatrix} Y^*_{1} \\ Y^*_{2} \\ \vdots \\ Y^*_{N} \end{bmatrix}\] <h2 id="fit-prophet-into-the-framework">Fit Prophet into the Framework</h2> <p>Prophet has three main components: trend, seasonality and holidays. Trend is a major driver of the model. Prophet is a special algorithm in the sense that once a model is fitted, the trend has a constant slope and thus the Prophet model is blind, insensitive to the modifications we do to the input vector (adding a residual to the forecast). In order to generate the path one has to refit Prophet every time we update the input vector in order for the trend slope to be updated accordingly. But this approach is time consuming because the model needs to be fitted as many times as the number of time points in the forecasting horizon.</p> <p>To avoid that we make an approximation. We assume that the perturbation of the forecast affects only the trend component, others remain the same. We also need to remember how a trend is learned. First Prophet identifies change points in the original time series and the trend is simply a piecewise linear curve connecting all pairs of neighboring change points. So the slope of the trend is allowed to change only after it passes the change point otherwise it is constant.</p> <p>We utilize these ideas to generate a future path in the case of the Prophet model. First we need to come up with a way to check whether a simulated point is a change point or not. To do that we look at the change points Prophet identified from the original time series and calculate the smallest change in the trend components observed at those historical change points. After we simulate a future value, say, \(y^*_{T+1}\) , we check whether the previous point (in this case it would be \(y_T\), this is not a simulated point as it is the last value in the time series) is a change point. It is a change point if a change in trend component at that point is greater or equal to the observed smallest change at the historical change points. In such case trend is updated and becomes a line connecting \(y_T\) and \(y^*_{T+1}\) Otherwise the slope of the trend remains the same. This is how the trend is simulated for future values. In order to reconstruct original values \(y_t\) we add seasonality and holiday components to the simulated trend values (remember we assume that only trend is affected in the path simulation process and seasonality and holiday components are already estimated by Prophet for the entire horizon).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/prediction_intervals/Prophet_Trend.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="simulations">Simulations</h2> <p>Here we show some examples for prediction intervals constructed for the Prophet model. The first graph simply shows simulated paths for a time series. The second is with prediction intervals without any aggregation for the same time series. The next graph shows prediction intervals for annual aggregation. The fourth one shows prediction intervals for the forecasts that are the sum of the forecasts computed by different multiple models. And finally prediction intervals in the case of both types of aggregations: sum of the annual forecasts coming from different models.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/prediction_intervals/SimulatedPaths.png" class="img-fluid rounded medium-zoom-image large-image" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig1. Simulated paths </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/prediction_intervals/PredictionInterval_Positive.png" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig2. Prediction intervals before aggregation: solid blue line - actual values, dotted red - forecast, dotted green - 5-th percentile, dotted purple - 95-th percentile </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/prediction_intervals/PredictionInterval_TimeAggregated.png" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig3. Prediction intervals after annual aggregation: solid blue - aggregated forecast, dotted red - 5-th percentile, dotted green - 95-th percentile </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/prediction_intervals/PredictionInterval_TimeSeriesAggregated.png" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig4. Prediction intervals after aggregating multiple forecasts of different models: solid blue line - actual values, dotted red - forecast, dotted green - 5-th percentile, dotted purple - 95-th percentile </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/prediction_intervals/PredictionInterval_Time_TimeSeries_Aggregated.png" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig5. Prediction intervals after aggregating annual forecasts of different models: solid blue - aggregated forecast, dotted red - 5-th percentile, dotted green - 95-th percentile </div> <h2 id="summary">Summary</h2> <p>The benefit of simulated path based approach for calculating prediction intervals is that it is distribution-free, generalizable for aggregated forecasts, easy to implement and it is fast. What it lacks is drift adaptation mechanism. We also made an approximation but that is Prophet specific. No need to make similar approximation if another algorithm is used for time series modeling.</p>]]></content><author><name>Andro Sabashvili</name></author><summary type="html"><![CDATA[A framework for constructing prediction intervals]]></summary></entry></feed>