<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://androsaba.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://androsaba.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-06-07T12:00:49+00:00</updated><id>https://androsaba.github.io/feed.xml</id><title type="html">blank</title><entry><title type="html">Forecasting</title><link href="https://androsaba.github.io/blog/2024/forecasting_benchmarks_practice/" rel="alternate" type="text/html" title="Forecasting"/><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://androsaba.github.io/blog/2024/forecasting_benchmarks_practice</id><content type="html" xml:base="https://androsaba.github.io/blog/2024/forecasting_benchmarks_practice/"><![CDATA[<h2 id="benchmarks">Benchmarks</h2> <h3 id="critique-of-prophet">Critique of Prophet</h3> <p>My motivation to carry out the benchmark for time series algorithms stemmed from realizing that a decent number of time series experts strongly dislike Prophet. Their main argument is that it’s not an autoregressive model, being trend plus seasonality instead. We know many real-life processes are autoregressive, like stock prices or exchange rates - important processes that these experts think an algorithm must account for to be useful.</p> <p>When researching this topic I came across the case of Zillow, the US real estate company that buys and sells homes. In 2021, they faced a major $300 million loss over a short period, which they attributed to failing to generate good forecasts. Consequently, they fired many employees.</p> <p>The connection between this fiasco and Prophet was a Zillow job listing seeking someone skilled in using Prophet. Those criticizing Prophet jumped to conclude that their losses resulted from using Prophet for forecasting. Of course, others argued Zillow used complicated neural network or machine learning models, but this view did not gain traction. Even today, the Zillow case is frequently cited when arguing against Prophet.</p> <p>After reading this, I wondered if I could find literature evidence that Prophet truly performs poorly. Surprisingly, such cases were easy to find.</p> <p>But what really convinced me was the statement in Hyndman’s book - Forecasting: Principles and Practice, 3rd edition. He writes, “However, [Prophet] rarely gives better forecast accuracy than alternative approaches.”</p> <p>Peter Cotton, another expert, has a blog post discussing Prophet’s shortcomings in detail while suggesting modifications to improve it. There’s even a tweet from Prophet’s creator, Sean Taylor, largely agreeing with Cotton’s criticisms.</p> <p>Cotton runs microprediction.com, hosting a leaderboard ranking Python time series forecasting packages where Prophet scores 1600 while the top package scores 2100, showing a significant performance gap between Prophet and the leading algorithms.</p> <h3 id="nixtla">Nixtla</h3> <p>So if Prophet underperforms, which package should one use instead? In my opinion, the answer is Nixtla. Their documentation is among the best I’ve seen - very organized, making it easy to find answers when starting to use their packages.</p> <p>We should discuss the benchmarks Nixtla performed against the popular Prophet package. As a time series startup in California, outperforming the widely-adopted Prophet is crucial for gaining traction.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/forecasting_benchmarks_practice/image1.jpg" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Their results show Nixtla’s implementation of statistical algorithms significantly outpacing Prophet in execution time across the M3 and M4 time series competition datasets, which use real-world data. For accuracy measured by mean absolute percentage error, Nixtla’s reimplementation often exceeds Prophet, though there are cases where Prophet performed better.</p> <p>Comparing the neural network extension “NeuralProphet” to the vanilla Prophet highlights even starker speed advantages for Nixtla. The left chart below compares execution time. Note that we have a logarithmic scale here. What does this mean? A small difference on a log scale represents a huge difference on the original scale. They chose to present this comparison on a log scale because on the original scale, the blue bars are so small as to be nearly invisible. But that makes sense, since it’s based on a neural net, which is much slower than regression. The right chart below compares accuracy. We see the same story - Nixltla generally outperforms Prophet.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/forecasting_benchmarks_practice/image9.jpg" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="nixtla-vs-prophet">Nixtla vs Prophet</h3> <p>This section is basically about verifying what I just discussed on real-world data. At the company I work for, we have monthly sales data from stores across shopping malls in many countries. Imagine a company owning numerous malls, and you have sales data from every store in each mall - stores spanning fashion, restaurants, entertainment, a wide variety of domains.</p> <p>While it’s simple monthly sales data, the number of time series I’m analyzing is roughly 3,000. A decent size, I would say, to conduct a solid benchmark.</p> <p>What I’m going to do is compare Nixtla and Prophet based on this real-world monthly sales sample. I’ll start with the most interpretable and easy-to-understand metric: the Relative Mean Absolute Error.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/forecasting_benchmarks_practice/image8.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Here, the numerator is the difference between the actual value $y_\tau$ and the forecast value, $\hat{y_\tau}$, which refers to either ARIMA, ETS, or CES forecasts. In the denominator, we have the Mean Absolute Error of the actual observed value, and forecast by Prophet, $\hat{y}^{base}$.</p> <p>Basically, if that ratio is less than one, it means the numerator model (ARIMA, ETS, or CES) performed better. If greater than one, the denominator (Prophet) performed better.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/forecasting_benchmarks_practice/image12.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>You see these distributions are all less than one on average, meaning all three models did a better job than Prophet on this specific data.</p> <p>The next few graphs look at the same point from different perspectives. Here I wanted to examine different metrics - RDM, which stands for Relative Deviation of Means (see the formula definition provided, as it’s not a common metric in the literature, but one we use internally).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/forecasting_benchmarks_practice/image3.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/forecasting_benchmarks_practice/image6.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>From the RDM comparison, we again see ARIMA, ETS, and CES performing better on average than Prophet. Though CES is closer to Prophet in MAE, the clear difference originates from ETS and ARIMA models. The same pattern holds for MAPE (Mean Absolute Percentage Error) and scaled MAE as well.</p> <p>Another interesting aspect is identifying the time series where Prophet performs best. The histograms below show this - for RDM, Prophet is best in 20% of cases; for MAE around 10-12%, and so on. On average, Prophet does a good job in roughly 13-14% of cases.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/forecasting_benchmarks_practice/image7.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The box plots below show the performance when Prophet is the best model. As you can see, when it tops the others, it actually does quite well comparatively.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/forecasting_benchmarks_practice/image13.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The next graph looks at how the metric values distribute across buckets. The vertical axis shows the metric buckets, while the circle diameters denote the number of time series falling in each corresponding bucket (though raw numbers aren’t provided).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/forecasting_benchmarks_practice/image14.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The picture remains consistent - Prophet skews lower, while the three Nixtla models shift upwards. There are no Prophet observations in the highest performance bucket, though the other models sometimes appear there.</p> <p>Essentially, across all metrics examined, the same story emerges: Prophet performs worse on average than Nixtla’s implementations of these statistical algorithms.</p> <p>I haven’t yet mentioned execution time - which is perhaps the biggest point here. As you saw in Nixtla’s benchmarks, they are the clear winner in execution speed. I observed the same in my analysis. To give you an idea, I had to build thousands of models for this project. Prophet has many hyperparameters to tune for optimization. Doing so for that many time series took almost two full days on my local machine.</p> <p>In contrast, using Nixtla on the exact same number of time series and procedure took only about 20-30 minutes.</p> <p>The takeaway is that if you only have one or two time series, using Prophet and at least trying it is reasonable. But it still makes sense to try simpler traditional statistical algorithms too. However, if you need to scale up to thousands or tens of thousands of time series, I don’t think anyone should use Prophet because, as mentioned, it’s very expensive computationally. For example, the M5 competition contained 100,000 time series. You can imagine how impractical using Prophet would be for a competition at that scale.</p> <h1 id="foundation-models">Foundation Models</h1> <p>Inspired by large language models people started training transformer based architectures on huge time series datasets. Nixtla was the first to train first foundation model for time series called TimeGPT.</p> <p>The key idea is - why would you want to have this pre-trained large model for time series? The reason is that with such a pre-trained model, you can perform what’s called zero-shot inference. This means you can take the pre-trained model like TimeGPT, pass in your time series data, and it will generate forecasts without any additional training. This zero-shot inference capability is very powerful and useful.</p> <p>At some point, once these foundation models become highly accurate and reliable, we’ll likely all use them routinely just as we do large language models today.</p> <p>Here’s a list of the foundation models for time series released since just last December:</p> <ul> <li><strong>TimeGPT</strong></li> <li><strong>Lag-Llama</strong></li> <li><strong>Moment</strong></li> <li><strong>TimesFM</strong></li> <li><strong>Moirai</strong></li> <li><strong>UniTS</strong></li> <li><strong>Chronos</strong></li> </ul> <p>Essentially, any company with expertise and resources is trying to train these GPT-like models for the ubiquitous time series domain.</p> <p>I’ve included some benchmarks comparing the foundation models’ performance. The left table below is for TimeGPT, with the last row showing its results. Bold numbers indicate top performance, while underscores denote second place. You can see TimeGPT is consistently in the top three. However, this benchmark was conducted by Nixtla themselves, so we’d like to see independent evaluations. The right table represents benchmark for the Lagllama model, with sentences highlighted to draw attention. They claim a simple seasonal naive model cannot outperform Lagllama, which is quite surprising.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/forecasting_benchmarks_practice/image15.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Nixtla also benchmarked Amazon’s model, finding that a simple statistical ensemble of ARIMA, ETS and CES actually performed better than this expensive model.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/forecasting_benchmarks_practice/image4.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The same holds for Salesforce - they were outperformed by a basic ensemble.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/forecasting_benchmarks_practice/image2.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The table below shows benchmark published by Amazon’s team itself.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/forecasting_benchmarks_practice/image10.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Based on this table, you can see that Chronos outperforms the statistical ensemble shown earlier.</p> <p>So there’s definitely competition and a race between companies to prove they have the state-of-the-art foundation model, having invested millions to train these massive models on billions of data points. In short, it’s an extremely new field and we don’t yet know what will happen or which approach will prove superior. But I think by year’s end we’ll have a clearer picture of whether foundation models provide value for time series, and which one to potentially trust - Nixtla, Amazon, or someone else.</p> <h1 id="best-practice">Best Practice</h1> <p>This is the second and final part of this post. We’ll discuss common mistakes data scientists often make when dealing with time series. The reason, I believe, is that time series is a very specialized domain. Most of data scientists likely come from diverse backgrounds outside statistics or economics, without formal university training in properly handling time series problems. Therefore, we lack the systematic knowledge required, which is why discussing some common pitfalls is useful.</p> <h2 id="benchmark">Benchmark</h2> <p>The first point is that you should always benchmark your algorithms or models. A famous example involves the numerous papers published frequently, where researchers design fancy deep learning architectures to forecast stock prices - a lucrative problem if solvable. However, we know stock prices follow a random walk process, where the future value equals the current value plus random noise.</p> <p>You’ll find papers lacking benchmarks against a naive forecast, which is theoretically the best model for a random walk, simply persisting the last observed value as the forecast. This is the simplest possible model, yet researchers fail to compare against it, despite trying to model a random walk. The chart below shows a neural network forecast (green line) exhibiting no significant difference from the naive forecast (red line) over the horizon.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/forecasting_benchmarks_practice/image5.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The point is - you should always try to choose an appropriate simple benchmark relevant to the problem, to ensure you’re making reasonable progress. Another issue is testing new algorithms on just 5-10 datasets, which is insufficient.</p> <h2 id="leakage">Leakage</h2> <p>Another classic point is avoiding data leakage - we all know this means processing data only after splitting training/test sets, not before. It can also happen when choosing the wrong forecast horizon. This one is more subtle yet commonly mistaken. Let’s say you want a 1-day ahead forecast model, and create a daily average temperature feature during development. But then the model gets deployed, and a user tries generating a forecast at 2pm while the day isn’t over yet. To compute that daily average temperature at 2pm, they’ll only have partial day data available. However, during training, you computed that feature using the entire day’s data. This results in leakage by using future observations for the training feature.</p> <p>The third point involves using one time series to forecast another with different lengths - e.g. the independent series has 12 months but the forecast target has 6 months. If not careful with aligning their time periods, future data can leak into the training set.</p> <h2 id="partitioning">Partitioning</h2> <p>Data partitioning for time series is also a common issue. We know standard k-fold cross-validation involving random shuffling is invalid, since time series data violates the i.i.d and exchangeability assumptions.</p> <p>To clarify, when I say “time series”, I don’t just mean a single variable sequence. Any 2D tabular data with a time dimension is effectively a time series problem requiring special handling. Some may not realize a multivariate table is still a time series case.</p> <p>For time series cross-validation, there are two approaches - fixed origin and rolling origin. Fixed origin means choosing a specific point like Jan 1, 2024 to split training data before that point from the test set after. Rolling origin means incrementing that split point forward through time - Jan 1, Feb 1, Mar 1, etc.</p> <p>There are multiple options for the rolling origin scenario. The first one is the expanding window. This means that when the origin moves into the future, the training sample gets larger and larger. Because if I started on January 1st, and now I move to February 1st, the entire January has already become part of my training sample. That’s why it’s called an expanding window. The size of the test sample is usually fixed in every scenario. However, in a rolling window, you fix your training size, you fix your test size, and you slide these windows across the time axis. So, the size of the training partition remains constant in a rolling window case.</p> <p>You can start with an expanding window and switch to a rolling window, or you can do a combination of both, depending on your use case and the size of the time series that you have.</p> <p>The second point is about standard K-fold cross-validation (CV). I was really surprised when I found out that I could use traditional random shuffling in the case of time series. I didn’t know about this. There are lots of conditions to be met to be able to use standard K-fold for time series. The series has to be stationary, and there should be no correlation between the different folds. The model that you are using should be able to deal with missing values. There are a bunch of conditions. If you are interested in this, have a look at <em>A note on the validity of cross-validation for evaluating autoregressive time series prediction</em> by Bergmeir, et al. But just so you know, you can apply standard CV for time series as well if these conditions are met.</p> <p>The last piece is about how we should do the data partitioning. If we have a short time series and I can make it stationary somehow, then you can use standard cross-validation for that scenario. But in other cases, you should stick with the time series CV. Again, if I have a long and a lot of data, then a rolling window is nice. If it’s short, then we want to start with expanding because if you have a short time series or a small number of data points, you don’t want to lose your points initially when you start your training. So it’s better to employ the expanding window so that you can accumulate all the future points in your training sample. And then you can switch to the rolling window, it’s up to you if you want to do that.</p> <h2 id="metric">Metric</h2> <p>If you’re familiar with the time series literature, you’ll remember that there are a bunch of metrics in forecasting literature, a lot of them. One can summarize them, I think, based on this table, at least this table helps me. There are three dimensions in the metrics usually: error, scaling, and aggregation.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/forecasting_benchmarks_practice/image16.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The error is just a difference between the actual value and the forecast, but we have to decide, do we want to take the square or just an absolute value? So we have two values in this dimension.</p> <p>Then we usually do the scaling. Why do we do the scaling? Because most of the time we have a lot of time series in this data sample and they have different scales. One could range from, I don’t know, 10 to 100 and another could range from 10,000 to 100,000. So in order to be able to compare these time series on different scales, we need to do the scaling to the metric. There are a bunch of different types of scaling you can apply.</p> <p>For the dataset, what does it mean? If you want to scale, you have to calculate the scaling factor based on your sample. And the question is, on which level should I compute that scaling factor? Is it on the entire dataset, entire series, per step, or relative? So relative is, for example, MAPE is a relative metric because it’s a ratio of the errors.</p> <p>There is an example of out-of-sample, what does this mean? Usually when they scale the metric, they want to use the base forecaster, like I did in the case of Prophet, Prophet in my calculation was the base forecaster. And they usually use this base forecaster’s errors to scale their original metric. And then you have to decide what kind of errors you want to use, in-sample or out-of-sample, again, depends on your time series.</p> <p>And finally, all of these are aggregated because we do this comparison for multiple points in time. We are not usually comparing just one forecast. We typically do forecasting, let’s say, for the next seven days or the next 12 months. So, in order to get a single number, I have to aggregate all these 12 numbers if I do a monthly forecast, for example. We can use the mean, median, or geometric mean. The combination of these three dimensions, different combinations, usually creates a bunch of time series metrics.</p> <h3 id="points-to-choose-right-meeasure">Points to Choose Right Meeasure</h3> <p>We know that squaring is related to optimizing the mean, while taking the absolute value optimizes the median, right? We know this from machine learning textbooks. Some people say that using a squared error for optimization during the training, and then using the mean absolute error when we evaluate the models, doesn’t make sense because algorithm optimizes the mean and not the median.</p> <p>Use scale-dependent metrics only for time series with the same scale.</p> <p>Evaluating your models using multiple metrics is a good practice because you can convince yourself that the model you trained is the best one on multiple different levels. That’s why in papers where they discuss forecasts, usually there are a bunch of metrics that they compute for every model. When the dataset is large, usually all these metrics agree with each other.</p> <p>R-squared is a very popular, widely used metric in papers and textbooks. But it’s a dangerous one because the purpose of R-squared is slightly different than evaluating forecasts.</p> <p>MAPE (Mean Absolute Percentage Error) is also very popular. However, it’s not a good idea to use it for small values because MAPE involves a ratio. When you divide by a very small number, it just blows up and doesn’t make sense.</p> <p>When we have seasonal or heteroscedastic data, it’s not a good idea to use percentage errors. You may have a big peak or dip, so you end up dividing by either a large or small number. It’s better to use per-series/global scaling to address this issue.</p> <p>For series with trends or structural breaks, using a scaled metric may not be reasonable. A structural break refers to when something happened that caused an abrupt shift in the process, like during COVID-19 when processes suddenly stopped. Computing the scaling factor depends on which time points you use. If it includes COVID-19 data, then that scaling won’t be relevant for other periods. So for trends/breaks, it’s better to use per-step scaling rather than per-series or per-sample scaling.</p> <p>The third point I already mentioned earlier - you have to decide whether to use in-sample or out-of-sample errors from your base forecaster for scaling. If the test sample is quite different from the training data, then the errors will differ substantially, so you need to carefully select which errors to use for the scaling factor.</p> <p>Use per-series scaling only for scenarios with no trend. When you have a strong increasing or decreasing trend, computing one scaling factor for the entire series isn’t useful, as you’re applying the same factor to points that may be very different in magnitude. It’s better to scale per step in trending cases.</p> <p>Regarding outliers, it’s well known that using the average is not ideal. Instead, use the median or absolute errors instead of squared errors to reduce the impact of outliers.</p> <p>Below is the useful diagram I recently found that starts with a question about your time series and, based on your answers, recommends which type of metric you should use. It can serve as a handy guide.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/forecasting_benchmarks_practice/image11.jpg" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="do-not-trust-plots">Do Not Trust Plots</h3> <p>The next point is about judging your forecast based on plots. I’ve seen many people doing this, and it’s not a good idea. For example, we have multiple models trained here - ETS (blue), naive fixed origin (green), and naive rolling origin (red).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/forecasting_benchmarks_practice/image17.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Looking at the plot, you might think the blue ETS line isn’t good because there’s a big difference from the actual data (black line). You might conclude the red line is best. However, looking at the table, ETS (blue) is actually the best performer.</p> <p>The reason for this discrepancy is that we tend to focus on the small horizontal displacements between the forecast and actual lines, which makes the red line seem closest. But what really matters is the vertical displacement or errors.</p> <p>The same applies to this random walk example with very simple straight line forecasts. You might think the forecasts can’t be so trivial and choose the yellow or purple line because they have some trend. But again, the table shows the naive model is best.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/forecasting_benchmarks_practice/image18.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The key takeaway is - we should always judge forecast accuracy based on the metrics, not visual plots alone.</p> <h3 id="statistical-significance">Statistical Significance</h3> <p>When benchmarking, we really want to know whether the observed performance differences are statistically significant or just coincidences from that particular sample realization. Usually, we perform statistical tests to ensure credible results.</p> <p>This simple diagram recommends tests based on whether comparing two models or more, and whether the normality assumption holds for the errors. For two models, use the t-test if normality holds, otherwise the Wilcoxon test. For more than two models, use ANOVA (F-test) if normality holds, otherwise the Friedman test.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/forecasting_benchmarks_practice/image19.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>However, tests like Diebold-Mariano and Wilcoxon are designed to compare just two forecast values at a specific time point. In practice, we usually generate multiple forecasts, so need to apply a correction like Bonferroni when performing multiple comparisons to control overall significance levels.</p> <p>So in summary - avoid being misled by visual plots, use appropriate statistical tests to rigorously assess significance of performance differences, and apply multiple comparison corrections when evaluating across many forecast points or time periods.</p>]]></content><author><name>Andro Sabashvili</name></author><summary type="html"><![CDATA[Benchmarks and Practice]]></summary></entry><entry><title type="html">Kranzberg’s Laws</title><link href="https://androsaba.github.io/blog/2024/kranzbergs_laws/" rel="alternate" type="text/html" title="Kranzberg’s Laws"/><published>2024-04-10T00:00:00+00:00</published><updated>2024-04-10T00:00:00+00:00</updated><id>https://androsaba.github.io/blog/2024/kranzbergs_laws</id><content type="html" xml:base="https://androsaba.github.io/blog/2024/kranzbergs_laws/"><![CDATA[<p><strong>First Law : Technology is neither good nor bad; nor is it neutral.</strong><br/> Everything is relative - Technology has complex and unpredictable impacts on the social ecology, requiring careful evaluation of its costs and benefits in different contexts and time frames.</p> <p>By that I mean that technology’s interaction with the social ecology is such that technical developments frequently have environmental, social, and human consequences that go far beyond the immediate purposes of the technical devices and practices themselves, and the same technology can have quite different results when introduced into different contexts or under different circumstances.Many of our technology-related problems arise because of the unforeseen consequences when apparently benign technologies are employed on a massive scale. Unforeseen dis-benefits can thus arise from presumably beneficent technologies. nuclear technology offers the prospect of unlimited energy resources, but it has also brought the possibility of worldwide destruction. It should constantly remind us to compare short-term versus long-term results, the utopian hopes versus the spotted actuality, the what-might-have been against what actually happened, and the trade-offs among various goods and possible bads; All of this can be done only by seeing how technology interacts in different ways with different values and institutions, indeed, with the entire sociocultural milieu.</p> <p><strong>Second Law: Invention is the mother of necessity.</strong><br/> Technical innovations often need further inventions to be fully effective.</p> <p>Every technical innovation seems to require additional technical advances in order to make it fully effective. Many major innovations have required further inventions to make them completely effective, Thus, Alexander Graham Bell’s telephone spawned a variety of technical improvements, ranging from Edison’s carbon-granule microphone to central-switching mechanisms.</p> <p><strong>Third Law: Technology comes in packages, big and small.</strong><br/> Complex mechanisms are systems of interconnected components that depend on each other and any change in one part affects the others and requires adjustments. Therefore, we cannot analyze the components separately, but we have to consider how they interact with each other.</p> <p>The fact is that today’s complex mechanisms usually involve several processes and components. Radar, for example, is a very complicated system, requiring specialized materials, power sources and intricate devices to send out waves of the proper frequency, detect them when they bounce off an object, and then interpret them and place the results on a screen. In the book Networks of Power What I call packages Hughes more precisely and accurately calls systems; which he defines as coherent structures composed on interacting, interconnected components. When one component changes, other parts of the system must undergo transformations so that the system might continue to function. Hence the parts of a system cannot be viewed in isolation but must be studied in terms of their interrelations with the other parts</p> <p><strong>Fourth Law: Although technology might be a prime element in many public issues, nontechnical factors take precedence in technology-policy decisions.</strong><br/> Technology-policy decisions depend more on nontechnical (social and political factors as well as the public’s perception of risk) factors than technology. No risk is the highest risk of all.</p> <p>In Networks of Power Hughes likewise demonstrates how nontechnical factors affected the efficient growth of electrical networks by comparing developments in Chicago, Berlin, and London. Private enterprise in Chicago, in the person of Samuel Insull, followed the path of the most efficient technology in seeking economies of scale. In Berlin and London, however, municipal governments were more concerned about their own authority than about technical efficiency, and political infighting meant that they lagged behind in developing the most economical power networks. Especially politicized has been the question of nuclear power. The nuclear industry itself has been partly to blame for technological deficiencies, but the presumption of risk by the public, especially following the Three Mile Island and Chernobyl accidents, has affected the future of what was once regarded as a safe and inexhaustible source of power. The public fears possible catastrophic consequences from nuclear generators. Yet the historical fact is that no one has been killed by commercial nuclear power accidents in this country. Contrast this with the 50,000 Americans killed each year by automobiles. Partly this is due to the public’s perception of risk, rather than to the actual risks themselves. People seek a zero-risk society. But as Aaron Wildavsky has so aptly put it, “No risk is the highest risk of all”. </p> <p><strong>Fifth Law: All history is relevant, but the history of technology is the most relevant.</strong><br/> History, a key to an understanding of the future, is often taught without considering the role of technology, which has shaped the daily life, the arts, and the humanistic endeavors of people throughout history.</p> <p>History is one of the fundamental liberal arts and is essential as a key to an understanding of the future. Many of today’s students simply do not see the relevance of history to the present or to their future. I suggest that this is because most history, as it is currently taught, ignores the technological element. Similarly, social historians of the Annales school have stressed how technology set the patterns of daily life for the vast majority of people throughout history. Perhaps most guilty of neglecting technology are those concerned with the history of the arts. This might be because they regard technology solely in terms of mechanical devices and do not even begin to comprehend the complex nature of technological developments and their direct influences on the arts, to say nothing of their indirect influence on mankind’s humanistic endeavors. any historian of art or of the Renaissance should perceive that such artistic masters as Leonardo and Michelangelo were also great engineers. That relationship continues today as David Billington has shown in stressing the relationship of structural design and art.</p> <p><strong>Sixth Law: Technology is a very human activity - and so is the history of technology.</strong><br/> <em>Technology is created by humans and humans are changed by technology</em>. Technology is both a product and a factor of human development, and there is a mutual dependence and influence between them.</p> <p>A lady came up to the great violinist Fritz Kreisler after a concert and gushed, “Maestro, your violin makes such beautiful music”; Kreisler held his violin up to his ear and said, “I don’t hear any music coming our of it”. You see, the instrument, the hardware, the violin itself, was of no use without the human element. But then again, without the instrument, Kreisler would not have been able to make music. The history of technology is the story of man and tool - hand and mind - working together. If the hardware is faulty or if the software is deficient, the sounds that emerge will be discordant: but when man and machine work together, they can make some beautiful music.</p>]]></content><author><name>Andro Sabashvili</name></author><summary type="html"><![CDATA[Summary of Technology and History: Kranzberg's Laws by Melvin Kranzberg published in Technology and Culture in 1986]]></summary></entry><entry><title type="html">The Mythos of Model Interpretability</title><link href="https://androsaba.github.io/blog/2024/model_interpretability/" rel="alternate" type="text/html" title="The Mythos of Model Interpretability"/><published>2024-04-10T00:00:00+00:00</published><updated>2024-04-10T00:00:00+00:00</updated><id>https://androsaba.github.io/blog/2024/model_interpretability</id><content type="html" xml:base="https://androsaba.github.io/blog/2024/model_interpretability/"><![CDATA[<p><em>…we should stop acting as if our goal is to author extremely elegant theories, and instead embrace complexity and make use of the best ally we have: the unreasonable effectiveness of data.</em><br/> – Alon Halevy et al, The Unreasonable Effectiveness of Data</p> <p>Model interpretability means transparency (i.e., how does the model work?) and post hoc explanations (i.e., what else can the model tell me?).</p> <p>Transparency can be defined at the level of the entire model (simulatability), at the level of individual components such as parameters (decomposability), and at the level of the training algorithm (algorithmic transparency).</p> <p>Let’s characterize each of them:<br/> Simulatability — an interpretable model is a simple model, easy to simulate. Sufficiently high-dimensional models, unwieldy rule lists, and deep decision trees could all be considered less transparent than comparatively compact neural networks.</p> <p>Decomposability — each part of the model (input, parameter, and calculation) admits an intuitive explanation. Note that this notion of interpretability requires that inputs themselves be individually interpretable, disqualifying some models with highly engineered or anonymous features. The weights of a linear model might seem intuitive, but they can be fragile with respect to feature selection and preprocessing.</p> <p>Algorithmic transparency — refers to learning algorithm itself. Deep learning models are less transparent than linear models.</p> <p>Note that humans exhibit none of these forms of transparency.</p> <p><strong>Conclusion</strong><br/> Linear models are not strictly more interpretable than deep neural networks. With respect to algorithmic transparency, this claim seems uncontroversial, but given high-dimensional or heavily engineered features, linear models lose simulatability or decomposability, respectively. When choosing between linear and deep models, you must often make a tradeoff between algorithmic transparency and decomposability. This is because deep neural networks tend to operate on raw or lightly processed features. So, if nothing else, the features are intuitively meaningful. To get comparable performance, however, linear models often must operate on heavily hand-engineered features. In some cases, transparency may be at odds with the broader objectives of AI. As a concrete example, the short-term goal of building trust with doctors by developing transparent models might clash with the longer-term goal of improving health care.</p>]]></content><author><name>Andro Sabashvili</name></author><summary type="html"><![CDATA[Summary of the Zachary C. Lipton's paper with the same title]]></summary></entry><entry><title type="html">Decision Making Needs Modeling Intervention Effects</title><link href="https://androsaba.github.io/blog/2023/treatment_effect_modeling/" rel="alternate" type="text/html" title="Decision Making Needs Modeling Intervention Effects"/><published>2023-11-29T00:00:00+00:00</published><updated>2023-11-29T00:00:00+00:00</updated><id>https://androsaba.github.io/blog/2023/treatment_effect_modeling</id><content type="html" xml:base="https://androsaba.github.io/blog/2023/treatment_effect_modeling/"><![CDATA[<h2 id="problem-with-ml">Problem with ML</h2> <p>Predictive machine learning models are built to improve decision making process. Usually they are embedded in complex decision logic. There are many processes that are intervened by us based on the predictions our ML model makes. For instance, consider churn prevention problem.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/treatment_effect_estimation/churn.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We build a churn prediction model, then we score the entire customer base, customers with the highest churn scores are identified and customer experience team sends them relevant offers retain them. After receiving offers some of them do not churn but some leave anyway. Few months later the churn model needs to be retrained on the new sample where persuadable customers (the ones that were contacted and stayed) are labeled as 0, while the lost ones (the ones that were contacted but left) are labeled as 1. Therefore former gets low score whereas the latter gets high score. So the new model learns to stop calling persuadable customers and keep calling lost customers. None of them is a decision we want to make. What this use case teaches us is that</p> <div style="background-color: rgba(0, 0, 0, 0.0470588); text-align: center; vertical-align: middle; padding: 20px 0; margin: 0px 0 20px;"> <strong>ignoring interventions leads to wrong decisions.</strong> </div> <p>One should estimate the intervention effect or, in this case, probability of churn given intervention instead of simply retraining the model.</p> <p>In the following we review algorithms for modeling intervention (treatment) effects. But before that let’s talk about another problem of ML which is <em>“causation is not the same as correlation”</em>. Let’s demonstrate this in case of two very basic causal DAGs, Fork and Collider.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/treatment_effect_estimation/dags.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Fork describes a process when a a single variable <strong>Z</strong> is affecting both <strong>X</strong> and <strong>Y</strong> and therefore they appear correlated even though there is is no causal link between them.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/treatment_effect_estimation/fork.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The spurious correlation is gone only after conditioning both of them on <strong>Z</strong>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/treatment_effect_estimation/fork_conditioned.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In Collider <strong>X</strong> and <strong>Y</strong> are not causally linked but they both affect <strong>Z</strong>. One doesn’t observe any correlation between <strong>X</strong> and <strong>Y</strong>, as it should be.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/treatment_effect_estimation/collider.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>But conditioning them on <strong>Z</strong> leads to a large $R^2$.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/treatment_effect_estimation/collider_conditioned.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Therefore, ignoring causal links can also lead us to wrong decisions as they would be based on the spurious correlations.</p> <h2 id="definitions">Definitions</h2> <p>Estimating a treatment effect implies answering what if… question or, in technical terms, estimating the corresponding counterfactual. Counterfactual refers to the value of the parameter of interest if intervention had not occur. So in the case of the churn use case, that would be the scenario when we never communicate with the customers. Another important concept is the treatment effect, which is the difference between the value of the parameter for the treated unit and the value the parameter for the treated unit would have had if we had not applied any treatment, Eq. 1.</p> <p>\begin{equation} TE = Y_T(t=1;W=1)-Y_T(t=1;W=0) \end{equation}</p> <p><strong>Y</strong> is the variable of interest where <strong>T</strong> stands for treatment group, <strong>t</strong> is a binary variable indicating whether it is measured in pre ($t=0$)- or post-intervention ($t=1$) period and <strong>W</strong> is also binary telling us whether treatment is applied or not. So by definition the second term in Eq.1 is counterfactual. Usually we want to calculate conditional average treatment effect given in Eq. 2.</p> <p>\begin{equation} CATE = E[Y_T(t=1;W=1)-Y_T(t=1;W=0)|X] \end{equation}</p> <p>Another term for the treatment effect coming from the marketing literature is uplift. It is used in the context of the interventions applied to a customer base.</p> <h2 id="algorithms-for-treatment-effect-estimation">Algorithms for Treatment Effect Estimation</h2> <h3 id="difference-in-differences-did">Difference in Differences (DiD)</h3> <p>So basically problem of treatment effect reduces to the counterfactual estimation because the first term in Eq. 2 is observed. There are two naive approximations for the counterfactual. The first one is to replace it with the value of $Y_T$ before the intervention.</p> \[CATE = E[Y_T(t=1;W=1)-Y_T(t=0;W=0)|X]\] <p>But this is not a good approximation because it assumes that there is no trend or seasonality in the time series and it changes only due the intervention. The second approximation is to utilize the control group in the post intervention period.</p> \[CATE = E[Y_T(t=1;W=1)-Y_C(t=1;W=0)|X]\] <p>But this approximation assumes that the control group is very similar to the treatment group in the pre-intervention period and would have remained similar if no treatment had been applied. That is also a strong assumption. Usually what’s used in practice is the combination of these two approximations resulting in the <strong>DiD</strong> estimator. So, counterfactual term is approximated with</p> \[\begin{align*} E[Y_T(t=1;W=0)|X]=&amp;E[Y_T(t=0;W=0)|X]+\\ &amp;+(E[Y_C(t=1;W=0)|X]-E[Y_C(t=0;W=0)|X]). \end{align*}\] <p>Plugging it into the equation of <strong>CATE</strong> yields</p> \[\begin{align*} CATE = &amp;(E[Y_T(t=1;W=1)|X] - E[Y_T(t=0;W=0)|X])-\\ &amp;-(E[Y_C(t=1;W=0)|X]-E[Y_C(t=0;W=0)|X]). \end{align*}\] <p>The first term is the difference between $Y_T$ values before and after intervention which is the treatment effect plus the change that would have occurred anyway. To cancel out the latter component the second term is subtracted which is a difference between $Y_C$ values before and after intervention. In other words we assume that the natural change that would have occurred in the treatment group is the same as in the control group. What if treatment and control time series have different scales? If this is the case let’s approximate the counterfactual using</p> \[\begin{align*} E[Y_T(t=1;W=0)|X]=E[Y_T(t=0;W=0)|X]\cdot \frac{E[Y_C(t=1;W=0)|X]}{E[Y_C(t=0;W=0)|X]} \end{align*}\] <p>resulting in</p> \[\begin{align*} CATE &amp;= (E[Y_T(t=1;W=1)|X] - E[Y_T(t=0;W=0)|X])-\\ &amp;-\frac{E[Y_T(t=0;W=0|X)]}{E[Y_C(t=0;W=0|X)]}\cdot(E[Y_C(t=1;W=0)|X]-E[Y_C(t=0;W=0)|X]) \end{align*}\] <p>more general expression in a sense that it reduces to the previous formula if the expectation of the treatment and control group values before the intervention are equal. Hence, to resolve issue of having different scales in the treatment and control time series ratio between pre- and post-intervention control group values is used instead of the absolute difference.</p> <h3 id="synthetic-control-brodersen-et-al-annals-of-applied-statistics-2015">Synthetic Control <d-footnote>Brodersen et al., Annals of Applied Statistics (2015)</d-footnote></h3> <p><strong>DiD</strong> requires treatment and control time series to have parallel trends and, in general, to be similar. This is quite strong requirement which is dropped by the synthetic control approach. Instead of finding a control time series very similar to the treated one let’s train a model on the bunch of untreated time series with output variable being the treated time series before the intervention. If the trained model has a good performance and accurately predicts the treated time series it can serve us as the control group. The model predictions in the post-intervention period represents counterfactual estimations. This is a very trivial but powerful idea. We don’t have to find time series similar to the treatment group, the only requirement is that the intervention does not affect them and they need to be predictive of the treatment time series.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/treatment_effect_estimation/synthetic_control.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 1. Synthetic control method </div> <p>Fig. 1 depicts this idea. $Y$ is the treated time series while $X_1$ and $X_2$ are input to the model that tries to predict $Y$. The dashed vertical line denotes the intervention. The solid blue is the fitted line whereas dashed blue line represents forecasts after the intervention. Therefore <strong>CATE</strong> would be the difference between solid black and dashed blue lines.</p> <h3 id="2-model-approach-s-athey-and-g-w-imbens-machine-learning-methods-for-estimating-heterogeneous-causal-effects-stat-10505-2015b">2-Model Approach <d-footnote>S. Athey and G. W. Imbens, Machine learning methods for estimating heterogeneous causal effects, stat, 1050:5, 2015b.</d-footnote></h3> <p>Another way of estimating <strong>CATE</strong> is to train two models on the treatment and the control group samples, respectively. Their predictions are estimates of the two terms in the definition of <strong>CATE</strong> in Eq. 2. This method can be reduced to the 1-model approach by combining treatment and control samples and adding <strong>W</strong>=(0,1) to the combined sample as one of the features.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/treatment_effect_estimation/2_model_approach.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 2. Left: 2-model approach, Right: 1-model approach </div> <h3 id="target-transformation-approach">Target Transformation Approach</h3> <p>The disadvantage of the previous technique is that models have their own errors and after subtracting their predictions we are not guaranteed that those errors cancel each other. Most likely the treatment effect has even bigger error. To avoid it we introduce an approach which models <strong>CATE</strong> explicitly. Let’s first describe it in the case of the binary outcome followed by the general case.</p> <h4 id="binary-target-jaskowski-m--jaroszewicz-s-uplift-modeling-for-clinical-trial-data-icml-workshop-on-clinical-data-analysis-vol-46-2012">Binary Target <d-footnote>Jaskowski M., &amp; Jaroszewicz S., "Uplift modeling for clinical trial data." ICML Workshop on Clinical Data Analysis. Vol. 46. 2012</d-footnote></h4> <p><strong>CATE</strong> for the discrete target variable is defined in the following way</p> \[CATE = P(Y=1|X;W=1)-P(Y=1|X;W=0)\] <p>So it’s simply a probability of success in the treatment group minus the probability of success in the control group. Now introduce a new variable <strong>Z</strong></p> \[Z=Y\cdot W+(1-Y)\cdot (1-W).\] <p>Here is the list of all possible values of <strong>Z</strong>:</p> \[Z = \begin{cases} 1 &amp;\text{Y=1, $W=1 $}\\ 1 &amp;\text{Y=0, $W=0 $}\\ 0 &amp;\text{Y=0, $W=1 $}\\ 0 &amp;\text{Y=1, $W=0 $}\\ \end{cases}\] <p>Let’s compute the conditional probability of <strong>Z</strong> being 1.</p> \[P(Z=1|X)=P(Y=1|X;W=1)P(W=1|X)+P(Y=1|X;W=0)P(W=0|X)\] <p>Now we make an assumption of balanced treatment (whether an object is treated or not does not depend on $X$ and the probability of being treated is the same as the one of not being treated)</p> \[P(W=1|X)=P(W=0|X)=1/2\] <p>which simplifies the latter equation further and gives</p> \[P(Y=1|X;W=1)-P(Y=1|X;W=0)=2P(Z=1|X)-1\] <p>where left hand side is the definition of <strong>CATE</strong>. Therefore, modeling variable <strong>Z</strong> is the same as modeling uplift explicitly.</p> <h4 id="general-case-s-athey-and-g-w-imbens-machine-learning-methods-for-estimating-heterogeneous-causal-effects-stat-10505-2015b">General Case <d-footnote>S. Athey and G. W. Imbens, Machine learning methods for estimating heterogeneous causal effects, stat, 1050:5, 2015b.</d-footnote></h4> <p>In the previous section we made assumption of balanced treatment, meaning equal treatment and control groups, which, in general, doesn’t hold in practice. Also $W$ being independent of $X$ can’t be achieved unless we have completely randomized process. To drop that assumption consider another transformation of the outcome variable which does not have to be binary anymore.</p> \[Z=Y(W=1)\cdot \frac{W}{p(X)}-Y(W=0)\cdot \frac{1-W}{1-p(X)}\] <p>where</p> \[p(X)=P(W=1|X)\] <p>is a propensity score (this idea is similar to the inverse propensity weighting approach). Let’s compute the expectation value of <strong>Z</strong> as we did in the previous section.</p> \[E[Z|X]=\frac{E[Y(W=1)\cdot W|X]}{p(X)}-\frac{E[Y(W=0)\cdot (1-W)]}{1-p(X)}\] <p>Conditional Independence Assumption or Unconfoundedness (whether object is treated or not does not depend on the outcome)</p> \[W \perp (Y(W=1), Y(W=0))|X\] <p>reduces the equation to the definition of <strong>CATE</strong></p> \[E[Z|X]=E[Y(W=1)|X]-E[Y(W=0)|X].\] <p>Here we used the following relationship</p> \[E[W|X]=p(X).\] <p>Again, modeling <strong>Z</strong> gives the treatment effect directly.</p> <h2 id="summary">Summary</h2> <p>As we discussed in the introduction it is important to incorporate treatments in the modeling process otherwise we would make wrong decisions. This was the motivation for reviewing four different approaches for modeling the treatment effect. I think it is natural to group them into two different groups.</p> <table> <thead> <tr> <th>Uplift Estimation</th> <th>Uplift Prediction</th> </tr> </thead> <tbody> <tr> <td><strong>DiD</strong> Estimator</td> <td><strong>2-Model</strong> Approach</td> </tr> <tr> <td><strong>Synthetic Control</strong></td> <td><strong>Targete Transformation</strong> Method</td> </tr> </tbody> </table> <p><strong>DiD</strong> and <strong>Synthetic Control</strong> are used when intervention already occurred, post-intervention values of the treatment group are already observed and we want to estimate the effect. But if we want to predict the uplift before the intervention <strong>2-Model</strong> and <strong>Target Transformation</strong> approaches are relevant. And lastly, <strong>Synthetic Control</strong> is the only method out of these four which allows using time series of completely different variables than the one in the treatment group as a control and additionally, unlike <strong>DiD</strong>, it allows using a dissimilar time series of the same variable as a control.</p>]]></content><author><name>Andro Sabashvili</name></author><summary type="html"><![CDATA[Review of treatment effect estimation techniques]]></summary></entry><entry><title type="html">Cross-Validation Estimator</title><link href="https://androsaba.github.io/blog/2023/cross_validation_estimator/" rel="alternate" type="text/html" title="Cross-Validation Estimator"/><published>2023-10-23T00:00:00+00:00</published><updated>2023-10-23T00:00:00+00:00</updated><id>https://androsaba.github.io/blog/2023/cross_validation_estimator</id><content type="html" xml:base="https://androsaba.github.io/blog/2023/cross_validation_estimator/"><![CDATA[<h4 id="notations">Notations:</h4> <ul> <li>\(m\) - metric</li> <li>\(K\) - number of folds</li> <li>\(N\) - number of different training sample realizations (drawn from the true population)</li> <li>\(\overline{m^n}\) - average metric (over \(K\) folds) for a given training sample realization</li> <li>\(Var(m^n_k)\) - variance of a metric over \(K\) folds for a given training sample realization</li> <li>\(Var(\overline{m^n})\) - variance of an average metric (over different training sample realizations)</li> <li>\(Var(m)\) - True/population variance of a metric</li> </ul> <h2 id="bias">Bias</h2> <p>The goal of the cross-validation is to estimate the mean of a metric, \(\overline{m}\) and its variance, \(Var(m)\). Cross-validation gives a pessimistically biased estimate of a performance because most statistical models will improve if the training set is made larger. However, Leave-one-out cross-validation (LOOCV) is approximately unbiased, because the difference in size between the training set used in each fold and the entire dataset is only a single instance.</p> <h2 id="variance">Variance</h2> <p>Intra-fold variance of a metric \(Var(m^n_k)\) is affected by the sample size as well as how diverse the sample is or in other words how different are validation folds from each other. The folds can be quite different easily in the low data regime if there are some instabilities in the sample, like outliers (for instance, if a given validation fold is very different from the training fold the value of \(m^n_k\) can be very different from the other values and thus will increase its variance). To factor out the sample diversity/instabilities from the estimation of \(Var(m)\) cross-validation is repeated \(N\) times (\(N\) is a large number, say, 5000) with different training samples. As a result we get the \(N\) number of \(\overline{m^n}\) allowing us to compute the mean of means and the variance of means, \(Var(\overline{m^n})\) and that is what is usually referred to when discussing the variation of the CV estimator. If training samples are independent \(Var(\overline{m^n})\) is lower than that in the case of the correlated training samples based on the argument mentioned below.</p> <h2 id="effect-of-model-stability">Effect of Model Stability</h2> <p>Before we dive into the details one should mention that CV does a good job if we are dealing with large samples but small sample regime (e.g. 40-60 records) is a different story. Number of folds and instabilities have a prominent impact on the metric estimations in the latter scenario.</p> <p>Intuitively one thinks that with increasing \(K\) we get more overlapped training samples and thus almost the same models within CV, training folds become highly correlated. They make similar predictions (almost the same prediction if \(K\) is very large, for example, if \(K\) is the same as the number of data points which corresponds to LOOCV) for a given validation sample. Therefore these estimates of a metric are not independent, they are correlated and consequently lead to higher variance, \(Var(\overline{m^n})\) <a href="https://stats.stackexchange.com/questions/61783/bias-and-variance-in-leave-one-out-vs-k-fold-cross-validation/358138#358138">[ref]</a> (why the mean of a correlated data has larger variance than that of the independent one is explained <a href="https://stats.stackexchange.com/questions/223446/variance-of-the-mean-of-correlated-and-uncorrelated-data">here</a>).</p> <p>On the other hand one can show empirically that \(Var(\overline{m^n})\) doesn’t always increase with increasing \(K\) rather it depends on the stability of a model and a sample. If we are given a sample with no instabilities and a model is also stable<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> \(Var(\overline{m^n})\) goes down with increasing \(K\) (LOOCV gives minimum or almost minimum variance) (see Fig. 1) while for the unstable case it reaches minimum for some intermediate value of \(K\) (LOOCV is not minimum anymore) <a href="https://stats.stackexchange.com/questions/61783/bias-and-variance-in-leave-one-out-vs-k-fold-cross-validation/358138#358138">[ref]</a>. So the difference is in the high \(K\) region (see Fig. 2)<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/cross_validation_variance/CV_Variance_NoInstabilities_60points.png" class="img-fluid rounded medium-zoom-image large-image" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig1. Stable version: $Var(\overline{m^n})$ (left) and $Var(m^n_k)$ (right) as a function of $K$ </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/cross_validation_variance/CV_Variance_Instabilities_60points.png" class="img-fluid rounded medium-zoom-image large-image" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig2. Unstable version: $Var(\overline{m^n})$ (left) and $Var(m^n_k)$ (right) as a function of $K$ </div> <p>In general the variance is smaller in a stable environment compared to the unstable one. In the stable environment a given training sample is representative of the true distribution and thus \(\overline{m^n}\) is going to have similar values for different realizations of the training sample resulting in lower variance compared to the unstable scenario. In the latter case a given realization of the training sample is not representative of the underlying distribution. Imagine there are some instabilities in the sample, say, outliers. In the small data regime they may have a big influence on the model. For instance, if there are no outliers in the current realization of a sample (say, when \(n=1\)) we get stable model that behaves differently compared to the model we get, let’s say, in the next iteration (\(n=2\)) because in that realization outliers are present which is causing the fitted function to become more wavy/fluctuating. Due to that reason, outputs of these two models (\(n=1\) vs \(n=2\)) are going to be different and therefore there is larger difference between \(\overline{m^1}\) and \(\overline{m^2}\) leading to the higher variance, \(Var(\overline{m^n})\).</p> <p>In the low data regime \(Var(\overline{m^n})\) is large for smaller \(K\) values in both cases, stable and unstable environments. That is because when \(K\) is small the training fold becomes smaller and the model becomes very unstable as it is more sensitive to any noise/sampling artifacts in the particular training sample used <a href="https://stats.stackexchange.com/a/264721">[ref]</a>.</p> <table> <thead> <tr> <th>Model</th> <th>With Increasing K</th> </tr> </thead> <tbody> <tr> <td>Stable</td> <td>\(Var(m^n_k)\) increases, \(Var(\overline{m^n})\) decreases</td> </tr> <tr> <td>Unstable</td> <td>\(Var(m^n_k)\) increases, \(Var(\overline{m^n})\) has <strong>U</strong> shape (first goes down then goes up, like parabola)</td> </tr> </tbody> </table> <p>Now we try to explain why \(Var(\overline{m^n})\) goes (almost) monotonically down with respect to increasing \(K\) (even in high \(K\) region) for a stable model and why it goes up with increasing \(K\) (in high \(K\) region) in the case of an unstable model:</p> <h4 id="stable-model"><u>Stable Model</u></h4> <p>Intuitive explanation for decreasing variance \(Var(\overline{m^n})\) in case of the stable model is that going from \(K\) to \(K+1\) intra-fold variance \(Var(m^n_k)\) is increasing but the means (\(m^1\), \(m^2\),…, \(m^N\)) get closer to each other because the training samples gets bigger (more stable) so we get more similar estimates of a metric mean.</p> <h4 id="unstable-model"><u>Unstable Model</u></h4> <p>In case of the unstable model, initially variance is large because the training samples are very small and models are more unstable than for large \(K\) values. So increasing \(K\) makes models more stable, \(K\) is still small so intra-fold variance \(Var(m^n_k)\) is small and the means are closer to each other thus variance \(Var(\overline{m^n})\) goes down. At some point, for some intermediate value of \(K\) variance reaches its minimum and then it starts to go up. This happens because for larger \(K\) training samples become more correlated and given unstable environment (unstable model and/or sample with instabilities) intra-fold variances get very large and therefore the corresponding metric means get pushed further apart from each other. The models become more stable but it can’t cancel out the effect of increasing \(Var(m^n_k)\) leading to larger \(Var(\overline{m^n})\).</p> <h2 id="summary">Summary</h2> <p>For large samples the effect of stability is not significant. In a low data regime, whether a large number of folds is better or not depends on the stability of a model and a sample. In the case of a stable environment LOOCV is better than k-fold CV because of low variance and bias. In the case of instabilities some smaller number of folds seems to have lower variance than LOOCV. But one has to remember that the error of the estimator is a combination of bias and variance. So overall which one is better depends also on the bias term.</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1" role="doc-endnote"> <p><span class="footnote"> Variance of the cross validation estimator is a linear combination of three moments. The terms 𝜔 and 𝛾 are influenced by correlation between the data sets, training sets, testing sets etc. and instability of the model. A model is stable if its output doesn’t change when deleting a point from the sample. These two effects are influenced by the value of 𝐾 which explains why different datasets and models will lead to different behavior <a href="https://stats.stackexchange.com/questions/325123/why-does-k-fold-cross-validation-generate-an-mse-estimator-that-has-higher-bias/358504#358504">[ref]</a> </span> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2" role="doc-endnote"> <p><span class="footnote"> Getting the effect of increasing variance for large values of 𝐾 is very senstive to the sample size and numeber of outliers. If a sample or number of outliers is larger their effect cancel out each other.</span> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name>Andro Sabashvili</name></author><summary type="html"><![CDATA[Discussing the variance of the cross-validation estimator]]></summary></entry><entry><title type="html">Prediction Intervals for Aggregated Forecasts</title><link href="https://androsaba.github.io/blog/2023/prediction_interval_for_time_series/" rel="alternate" type="text/html" title="Prediction Intervals for Aggregated Forecasts"/><published>2023-10-09T00:00:00+00:00</published><updated>2023-10-09T00:00:00+00:00</updated><id>https://androsaba.github.io/blog/2023/prediction_interval_for_time_series</id><content type="html" xml:base="https://androsaba.github.io/blog/2023/prediction_interval_for_time_series/"><![CDATA[<h2 id="intro">Intro</h2> <p>I won’t go into details about why prediction intervals are important, we all know that. I just want to introduce a framework that will allow us to estimate a prediction interval for a single forecast, and then we will generalize it for aggregated forecasts. I started thinking about this problem when I was working on a sales forecasting model earlier this year. The model produced monthly forecasts, but the clients were interested in aggregated forecasts, such as annual sales of a store or forecasted annual sales for the entire shopping mall. I realized that I needed a way to estimate the prediction interval for aggregated forecasts, given that I only had a model that produced monthly forecasts.</p> <h2 id="distribution-free-approach">Distribution-free Approach</h2> <p>There are many approaches to building prediction intervals. Most of them are based on the residuals assumed that they are normally distributed. There are some fancy formulas that allow you to construct prediction intervals based on this normality assumption. However, we know that this assumption is often unrealistic and doesn’t hold in practice. I wanted to have a method that is distribution-free, so I decided to use a technique based on the bootstrapping approach. The basic idea is to simulate future paths of the variable of interest. This will allow us to estimate the distribution of the variable at any future time point. The technique can be used for supervised learning algorithms as well as time series algorithms.</p> <h2 id="simulating-future-paths">Simulating Future Paths</h2> <p>To generate a simulated path, we start with a forecast at the first time point in the horizon, \(T+1\), draw a random residual from the residual distribution and add it to the forecast. This gives us the first simulated value, \(y^*_{T+1}\). We then use the first simulated value as the input to the model to generate a forecast for the next time point in the horizon. We repeat this process until we have generated a sequence of simulated values for all of the future time points. This process is repeated multiple times to generate multiple simulated paths. This will give us a distribution of the variable of interesty at any future time point which is used to construct prediction intervals.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/prediction_intervals/Path_Equation.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="aggregation-across-different-dimensions">Aggregation Across Different Dimensions</h2> <p>We consider two types of aggregations:</p> <ul> <li>Aggregation across time axis is the process of combining forecasts for different time points into a single forecast. For example, if you have monthly forecasts for a year, you can aggregate them to get an annual forecast.</li> <li>Aggregation across time series is the process of combining forecasts for different time series into a single forecast. For example, if you have forecasts for sales of different products, you can aggregate them to get a forecast for total sales. Our method can be used to estimate prediction intervals for both, aggregation across time and aggregation across time series.</li> </ul> <h3 id="aggregation-across-time">Aggregation Across Time</h3> <p>For each simulated path, aggregate the monthly forecasts to obtain an annual forecast. This can be done by simply summing the monthly forecasts for each year (see equation below). As a result we are left with an array of \(N\) (number of simulated paths) elements for a given year, \((y_{1,year}, y_{2,year},..., y_{N,year})\). Estimate the lower and upper bounds of the prediction interval at the desired confidence level by computing \((1-\alpha)/2\)-th and \((1+\alpha)/2\)-th quantiles of the resulting distribution.</p> \[y^*_{s, 2027} = \sum^{12}_{m=1} y^*_{s,m/2027}\] <h3 id="aggregation-across-time-series">Aggregation Across Time Series</h3> <p>To estimate a prediction interval for an aggregated forecast across time series do a cross-section of the simulated paths for each time series at the time point of interest. Which gives us \(T\) (number of time series) arrays of length \(N\) (number of simulated paths in a single time series). Each array is a distribution of \(y\) at that time point. Therefore, we can say that we have a set of distributions of \(T\) random variables and the goal is to estimate the distribution of the sum of those random variables given that the distribution for each of them is known. One approach to do that is convolution but becomes infeasible when the number of random variables (time series) is large. More scalable and fast approach is Monte Carlo simulation: these arrays/distributions are put together in a matrix with columns being a distribution corresponding to a single time series. So, the matrix has an \(N \times T\) shape (\(N\) rows because there are \(N\) elements in a distribution since there are \(N\) paths, \(T\) columns - because we have \(T\) time series to aggregate). Then we sum the matrix entries horizontally resulting in the array of length \(N\) which is the estimation of the distribution of the aggregated forecasts. Estimate the lower and upper bounds of the prediction interval at the desired confidence level by computing quantiles of the resulting distribution.</p> \[\begin{bmatrix} y^{*,1}_{1} &amp; y^{*,2}_{1} &amp; y^{*,3}_{1} &amp; \cdots &amp; y^{*,T}_{1} \\ y^{*,1}_{2} &amp; y^{*,2}_{2} &amp; y^{*,3}_{2} &amp; \cdots &amp; y^{*,T}_{2} \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ y^{*,1}_{N} &amp; y^{*,2}_{N} &amp; y^{*,3}_{N} &amp; \cdots &amp; y^{*,T}_{N} \\ \end{bmatrix} \begin{matrix} \\ \stackrel{\sum_t}{\Longrightarrow} \\ \\ \end{matrix} \begin{bmatrix} Y^*_{1} \\ Y^*_{2} \\ \vdots \\ Y^*_{N} \end{bmatrix}\] <h2 id="fit-prophet-into-the-framework">Fit Prophet into the Framework</h2> <p>Prophet has three main components: trend, seasonality and holidays. Trend is a major driver of the model. Prophet is a special algorithm in the sense that once a model is fitted, the trend has a constant slope and thus the Prophet model is blind, insensitive to the modifications we do to the input vector (adding a residual to the forecast). In order to generate the path one has to refit Prophet every time we update the input vector in order for the trend slope to be updated accordingly. But this approach is time consuming because the model needs to be fitted as many times as the number of time points in the forecasting horizon.</p> <p>To avoid that we make an approximation. We assume that the perturbation of the forecast affects only the trend component, others remain the same. We also need to remember how a trend is learned. First Prophet identifies change points in the original time series and the trend is simply a piecewise linear curve connecting all pairs of neighboring change points. So the slope of the trend is allowed to change only after it passes the change point otherwise it is constant.</p> <p>We utilize these ideas to generate a future path in the case of the Prophet model. First we need to come up with a way to check whether a simulated point is a change point or not. To do that we look at the change points Prophet identified from the original time series and calculate the smallest change in the trend components observed at those historical change points. After we simulate a future value, say, \(y^*_{T+1}\) , we check whether the previous point (in this case it would be \(y_T\), this is not a simulated point as it is the last value in the time series) is a change point. It is a change point if a change in trend component at that point is greater or equal to the observed smallest change at the historical change points. In such case trend is updated and becomes a line connecting \(y_T\) and \(y^*_{T+1}\) Otherwise the slope of the trend remains the same. This is how the trend is simulated for future values. In order to reconstruct original values \(y_t\) we add seasonality and holiday components to the simulated trend values (remember we assume that only trend is affected in the path simulation process and seasonality and holiday components are already estimated by Prophet for the entire horizon).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/prediction_intervals/Prophet_Trend.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="simulations">Simulations</h2> <p>Here we show some examples for prediction intervals constructed for the Prophet model. The first graph simply shows simulated paths for a time series. The second is with prediction intervals without any aggregation for the same time series. The next graph shows prediction intervals for annual aggregation. The fourth one shows prediction intervals for the forecasts that are the sum of the forecasts computed by different multiple models. And finally prediction intervals in the case of both types of aggregations: sum of the annual forecasts coming from different models.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/prediction_intervals/SimulatedPaths.png" class="img-fluid rounded medium-zoom-image large-image" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig1. Simulated paths </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/prediction_intervals/PredictionInterval_Positive.png" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig2. Prediction intervals before aggregation: solid blue line - actual values, dotted red - forecast, dotted green - 5-th percentile, dotted purple - 95-th percentile </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/prediction_intervals/PredictionInterval_TimeAggregated.png" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig3. Prediction intervals after annual aggregation: solid blue - aggregated forecast, dotted red - 5-th percentile, dotted green - 95-th percentile </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/prediction_intervals/PredictionInterval_TimeSeriesAggregated.png" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig4. Prediction intervals after aggregating multiple forecasts of different models: solid blue line - actual values, dotted red - forecast, dotted green - 5-th percentile, dotted purple - 95-th percentile </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/prediction_intervals/PredictionInterval_Time_TimeSeries_Aggregated.png" class="img-fluid rounded" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig5. Prediction intervals after aggregating annual forecasts of different models: solid blue - aggregated forecast, dotted red - 5-th percentile, dotted green - 95-th percentile </div> <h2 id="summary">Summary</h2> <p>The benefit of simulated path based approach for calculating prediction intervals is that it is distribution-free, generalizable for aggregated forecasts, easy to implement and it is fast. What it lacks is drift adaptation mechanism. We also made an approximation but that is Prophet specific. No need to make similar approximation if another algorithm is used for time series modeling.</p>]]></content><author><name>Andro Sabashvili</name></author><summary type="html"><![CDATA[A framework for constructing prediction intervals]]></summary></entry></feed>